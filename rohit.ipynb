{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection using ResNet50\n",
    "This notebook implements a deep learning pipeline to identify metastatic cancer in small image patches taken from larger digital pathology scans. We use **Transfer Learning** with a ResNet50 backbone, followed by fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "We use TensorFlow/Keras for modeling and Scikit-Learn for data splitting and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "We load the CSV labels and create absolute paths for the images. The data is split into 80% training and 20% validation sets, stratified by the label to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('train_labels.csv')\n",
    "image_dir = 'train/'\n",
    "\n",
    "# Create image path column\n",
    "labels_df['image_path'] = labels_df['id'].apply(lambda x: os.path.join(image_dir, f\"{x}.tif\"))\n",
    "\n",
    "# Train-Validation Split (80/20 Stratified)\n",
    "train_df, val_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Augmentation\n",
    "To prevent overfitting and improve generalization, we apply random transformations like rotation, zoom, and horizontal flips. We use the `preprocess_input` function specific to ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=None, \n",
    "    x_col=\"image_path\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(96, 96),\n",
    "    batch_size=32,\n",
    "    class_mode='raw'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=None,\n",
    "    x_col=\"image_path\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(96, 96),\n",
    "    batch_size=32,\n",
    "    class_mode='raw'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Architecture (Transfer Learning)\n",
    "We load ResNet50 pre-trained on ImageNet. Initially, we freeze all ResNet layers and only train the custom head we added on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "base_model.trainable = False  # Freeze base layers initially\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Starting Initial Training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fine-Tuning\n",
    "Now we unfreeze the later stages of the ResNet50 model (Stage 5) to allow the model to learn features specific to histopathologic images. We use a very low learning rate to avoid destroying the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnfreezing top layers for Fine-Tuning...\")\n",
    "\n",
    "# Access layer before original head and rebuild slightly for better stability\n",
    "x = model.layers[-3].output \n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "new_predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Unfreeze the last block of ResNet50\n",
    "for layer in base_model.layers[:140]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[140:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "new_model = Model(inputs=model.input, outputs=new_predictions)\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for optimized training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-7)\n",
    "\n",
    "history_finetune = new_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Final Evaluation\n",
    "We visualize the training history and calculate the Precision and Recall on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy/Loss Plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_finetune.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_finetune.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_finetune.history['loss'], label='Train Loss')\n",
    "plt.plot(history_finetune.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision/Recall\n",
    "val_generator.reset()\n",
    "y_pred = new_model.predict(val_generator)\n",
    "y_true = val_df['label'].values\n",
    "\n",
    "precision = precision_score(y_true, (y_pred > 0.5).astype(int))\n",
    "recall = recall_score(y_true, (y_pred > 0.5).astype(int))\n",
    "\n",
    "print(f'\\nFinal Validation Precision: {precision:.4f}')\n",
    "print(f'Final Validation Recall: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}